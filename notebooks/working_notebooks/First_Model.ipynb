{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JCherryA050/phase_4_project/blob/main/First_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Planet Recommendation System With ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "Anime Planet is a website where users can track and rate the anime and manga that they consume. Using this information, Anime Planet has a global ranking of anime/manga, has a recommendation system based on users preferences, and directs users to where they can watch or read a given anime/manga.\n",
    "\n",
    "We have been tasked with taking user data specifically dealing with anime to find out if we can create a more effective recommendation system using Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8CMJxg1sqS2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Environment and Data\n",
    "\n",
    "Because ALS relies on a sparse matrix of data (even when cleaning the data to reduce the size), we will be taking advantage Google's cloud service Colab, which will allow us to process an extremely large dataset. Google Colab will also allow us to utilize PySpark, a tool for ALS models.\n",
    "\n",
    "To start, we will run the following to set up our Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnXxPBsVsqS4",
    "outputId": "4fee6050-f1af-43c4-9e0b-4c29e670fcfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
      "\u001b[K     |████████████████████████████████| 212.4MB 71kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 22.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=a9bb4728e4119634e5ac1f16d79ca533611d691889e53181e591e309bc1a6c2f\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.1.2\n",
      "The following additional packages will be installed:\n",
      "  openjdk-8-jre-headless\n",
      "Suggested packages:\n",
      "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
      "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
      "  fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
      "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
      "Need to get 36.5 MB of archives.\n",
      "After this operation, 143 MB of additional disk space will be used.\n",
      "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
      "(Reading database ... 160815 files and directories currently installed.)\n",
      "Preparing to unpack .../openjdk-8-jre-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
      "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jdk-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
      "Setting up openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
      "Setting up openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
      "Collecting mlflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/c9/190a45e667b63edb76112deefa70629c2d9985603a85cb1968015fe0f327/mlflow-1.18.0-py3-none-any.whl (14.2MB)\n",
      "\u001b[K     |████████████████████████████████| 14.2MB 221kB/s \n",
      "\u001b[?25hCollecting querystring-parser\n",
      "  Downloading https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.5)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/af/631375abc29e59cedfa4467a5f7755503ba19898890751e1f2636ef02f92/databricks-cli-0.14.3.tar.gz (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 10.6MB/s \n",
      "\u001b[?25hCollecting prometheus-flask-exporter\n",
      "  Downloading https://files.pythonhosted.org/packages/f3/c1/2cc385fadf18dc75fe24c18899269eda4dcc60221d61eff7da4a6cc5c01d/prometheus_flask_exporter-0.18.2.tar.gz\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.19.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow) (20.9)\n",
      "Collecting docker>=4.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/5a/f988909dfed18c1ac42ad8d9e611e6c5657e270aa6eb68559985dbb69c13/docker-5.0.0-py2.py3-none-any.whl (146kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 51.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.17.3)\n",
      "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
      "Collecting gunicorn; platform_system != \"Windows\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/dd/5b190393e6066286773a67dfcc2f9492058e9b57c4867a95f1ba5caf0a83/gunicorn-20.1.0-py3-none-any.whl (79kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 11.4MB/s \n",
      "\u001b[?25hCollecting gitpython>=2.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 54.7MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 49.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.20)\n",
      "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
      "Collecting alembic<=1.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/e9/359dbb77c35c419df0aedeb1d53e71e7e3f438ff64a8fdb048c907404de3/alembic-1.4.1.tar.gz (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 48.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2018.9)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from querystring-parser->mlflow) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mlflow) (2.8.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
      "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow) (2.4.7)\n",
      "Collecting websocket-client>=0.32.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5f/3c211d168b2e9f9342cfb53bcfc26aab0eac63b998015e7af7bcae66119d/websocket_client-1.1.0-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2021.5.30)\n",
      "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn; platform_system != \"Windows\"->mlflow) (57.0.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (3.7.4.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (4.6.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (1.1.0)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
      "Collecting Mako\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 13.1MB/s \n",
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy->mlflow) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow) (2.0.1)\n",
      "Building wheels for collected packages: databricks-cli, prometheus-flask-exporter, alembic\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.14.3-cp37-none-any.whl size=100560 sha256=21b089ca5392e3c04752cebef8052293332ee7a1323d9fe65f9d36cc275994de\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/24/f3/34d8e3964dac4ba849d844273c49a679111b00d5799ebb934a\n",
      "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-cp37-none-any.whl size=17415 sha256=ea40e15d7604824309b75b9a7930d8bca4ce452c375f8117296c548a9f5214dc\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/e2/9c/4f3ee23964802940f81a8b476d0b9be6fb6348cb12df2e2226\n",
      "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158170 sha256=e3bfb0360d350c64ac56ad4bccdbdc740ccbf6bc1243ca4781202bf38ce5ab88\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/07/f7/12f7370ca47a66030c2edeedcc23dec26ea0ac22dcb4c4a0f3\n",
      "Successfully built databricks-cli prometheus-flask-exporter alembic\n",
      "Installing collected packages: querystring-parser, databricks-cli, prometheus-flask-exporter, websocket-client, docker, gunicorn, smmap, gitdb, gitpython, pyyaml, Mako, python-editor, alembic, mlflow\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed Mako-1.1.4 alembic-1.4.1 databricks-cli-0.14.3 docker-5.0.0 gitdb-4.0.7 gitpython-3.1.18 gunicorn-20.1.0 mlflow-1.18.0 prometheus-flask-exporter-0.18.2 python-editor-1.0.4 pyyaml-5.4.1 querystring-parser-1.2.4 smmap-4.0.0 websocket-client-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Run for Google Colab environment\n",
    "!pip install pyspark\n",
    "!apt install openjdk-8-jdk-headless -qq\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have installed everything we need, we will import the following libraries for PySpark, and set up a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Cs3k8BdsstrD"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "# import org.apache.spark.sql.functions.col\n",
    "# import org.apache.spark.sql.types.IntegerType\n",
    "# import pyspark.sql.functions.col\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-q-0oziytrod"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('anime_rec').config('spark.driver.host', 'localhost')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to create a spark dataframe for our user recommendation data. We only want ratings, user id, and anime id. Because the dataset is already pretty large, we want to drop anything we can, so we can get rid of watched_status and watched_episodes. Additonally, we need to make sure all of our remaining values are integers.\n",
    "\n",
    "Note: If you are also using Google Colab, you will need to make sure to upload any CSVs in the colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WN6p3aIWtCOs"
   },
   "outputs": [],
   "source": [
    "rec_data = spark.read.csv('animelist.csv', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6M2a5rActmWK"
   },
   "outputs": [],
   "source": [
    "rec_data = rec_data.withColumn('rating', rec_data['rating'].cast(IntegerType()))\n",
    "rec_data = rec_data.withColumn('user_id', rec_data['user_id'].cast(IntegerType()))\n",
    "rec_data = rec_data.withColumn('anime_id', rec_data['anime_id'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rstj-AYJt_h3",
    "outputId": "f06ef096-6f31-4238-d263-0ff818cb5ee3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user_id', 'int'),\n",
       " ('anime_id', 'int'),\n",
       " ('rating', 'int'),\n",
       " ('watching_status', 'string'),\n",
       " ('watched_episodes', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mWi-85ODw8Va"
   },
   "outputs": [],
   "source": [
    "rec_data = rec_data.drop('watching_status')\n",
    "rec_data = rec_data.drop('watched_episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA6olC_TxEhG",
    "outputId": "4aa99013-f2d1-40a1-d72c-460a0b347948"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, anime_id: int, rating: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Our Model\n",
    "\n",
    "Now that everything is set up, we can do a train test split and build a model. Our first model will make some guesses for our parameters, and from there we can try tweaking things a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Y2fEfTHyxMuo"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "(training, test) = rec_data.randomSplit([0.8, 0.2], seed=1)\n",
    "\n",
    "als = ALS(maxIter=5, rank=10, regParam=0.01, userCol='user_id', itemCol='anime_id', ratingCol='rating', coldStartStrategy ='drop')\n",
    "# fit the ALS model to the training set\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ts3MoklAxpO6",
    "outputId": "922f1ffd-bf02-453e-d87a-128638b3469c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 3.13945031256278\n"
     ]
    }
   ],
   "source": [
    "# importing appropriate library\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('Root-mean-square error = ' + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Root Mean Square Error (RMSE) is at about 3, which means that our model can predict a user rating for a given anime within 3 points. We are using a 10 point scale, so this is too high an error to depend on. The next thing we can do is use cross validation to find better parameters to reduce our RMSE to be more dependable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Q0Hz1P9cKbaZ"
   },
   "outputs": [],
   "source": [
    "# Takes about an hour to run\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# initialize the ALS model\n",
    "als_model = ALS(userCol='user_id', itemCol='anime_id', ratingCol='rating', coldStartStrategy='drop')\n",
    "\n",
    "# create the parameter grid              \n",
    "params = ParamGridBuilder()\\\n",
    "  .addGrid(als_model.regParam, [0.01, 0.001, 0.1])\\\n",
    "  .addGrid(als_model.rank, [10]).build() # Ran earlier and found 10 to be best\n",
    "\n",
    "# instantiating crossvalidator estimator\n",
    "cv = CrossValidator(estimator=als_model, estimatorParamMaps=params, evaluator=evaluator, parallelism=4)\n",
    "best_model = cv.fit(rec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "7sBXkCXUkOr9",
    "outputId": "fafa6f4e-5e6e-463b-95c7-feee5ee735d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d1076c1e1900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We see the best model has a rank of 10, so we will use that in our future models with this dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'regParam' is not defined"
     ]
    }
   ],
   "source": [
    "# We see the best model has a rank of 10, so we will use that in our future models with this dataset\n",
    "print(best_model.bestModel.rank)\n",
    "print(best_model.bestModel.getParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching IDs to Names\n",
    "\n",
    "Now that we have a model with more dependable scores, we need to be a bit user friendly. It is not very helpful to recommend anime_id 1 to a user, rather than Cowboy Bepop!\n",
    "\n",
    "We are going to pull in our data set with anime information and write a function to get the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5xAjDm-KxaO",
    "outputId": "8d725967-28d1-4df4-c690-45024ad51d5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(MAL_ID='1', Name='Cowboy Bebop', Score='8.78', Genders='Action, Adventure, Comedy, Drama, Sci-Fi, Space', English name='Cowboy Bebop', Japanese name='カウボーイビバップ', Type='TV', Episodes='26', Aired='Apr 3, 1998 to Apr 24, 1999', Premiered='Spring 1998', Producers='Bandai Visual', Licensors='Funimation, Bandai Entertainment', Studios='Sunrise', Source='Original', Duration='24 min. per ep.', Rating='R - 17+ (violence & profanity)', Ranked='28.0', Popularity='39', Members='1251960', Favorites='61971', Watching='105808', Completed='718161', On-Hold='71513', Dropped='26678', Plan to Watch='329800', Score-10='229170.0', Score-9='182126.0', Score-8='131625.0', Score-7='62330.0', Score-6='20688.0', Score-5='8904.0', Score-4='3184.0', Score-3='1357.0', Score-2='741.0', Score-1='1580.0'),\n",
       " Row(MAL_ID='5', Name='Cowboy Bebop: Tengoku no Tobira', Score='8.39', Genders='Action, Drama, Mystery, Sci-Fi, Space', English name='Cowboy Bebop:The Movie', Japanese name='カウボーイビバップ 天国の扉', Type='Movie', Episodes='1', Aired='Sep 1, 2001', Premiered='Unknown', Producers='Sunrise, Bandai Visual', Licensors='Sony Pictures Entertainment', Studios='Bones', Source='Original', Duration='1 hr. 55 min.', Rating='R - 17+ (violence & profanity)', Ranked='159.0', Popularity='518', Members='273145', Favorites='1174', Watching='4143', Completed='208333', On-Hold='1935', Dropped='770', Plan to Watch='57964', Score-10='30043.0', Score-9='49201.0', Score-8='49505.0', Score-7='22632.0', Score-6='5805.0', Score-5='1877.0', Score-4='577.0', Score-3='221.0', Score-2='109.0', Score-1='379.0'),\n",
       " Row(MAL_ID='6', Name='Trigun', Score='8.24', Genders='Action, Sci-Fi, Adventure, Comedy, Drama, Shounen', English name='Trigun', Japanese name='トライガン', Type='TV', Episodes='26', Aired='Apr 1, 1998 to Sep 30, 1998', Premiered='Spring 1998', Producers='Victor Entertainment', Licensors='Funimation, Geneon Entertainment USA', Studios='Madhouse', Source='Manga', Duration='24 min. per ep.', Rating='PG-13 - Teens 13 or older', Ranked='266.0', Popularity='201', Members='558913', Favorites='12944', Watching='29113', Completed='343492', On-Hold='25465', Dropped='13925', Plan to Watch='146918', Score-10='50229.0', Score-9='75651.0', Score-8='86142.0', Score-7='49432.0', Score-6='15376.0', Score-5='5838.0', Score-4='1965.0', Score-3='664.0', Score-2='316.0', Score-1='533.0'),\n",
       " Row(MAL_ID='7', Name='Witch Hunter Robin', Score='7.27', Genders='Action, Mystery, Police, Supernatural, Drama, Magic', English name='Witch Hunter Robin', Japanese name='Witch Hunter ROBIN (ウイッチハンターロビン)', Type='TV', Episodes='26', Aired='Jul 2, 2002 to Dec 24, 2002', Premiered='Summer 2002', Producers='TV Tokyo, Bandai Visual, Dentsu, Victor Entertainment', Licensors='Funimation, Bandai Entertainment', Studios='Sunrise', Source='Original', Duration='25 min. per ep.', Rating='PG-13 - Teens 13 or older', Ranked='2481.0', Popularity='1467', Members='94683', Favorites='587', Watching='4300', Completed='46165', On-Hold='5121', Dropped='5378', Plan to Watch='33719', Score-10='2182.0', Score-9='4806.0', Score-8='10128.0', Score-7='11618.0', Score-6='5709.0', Score-5='2920.0', Score-4='1083.0', Score-3='353.0', Score-2='164.0', Score-1='131.0'),\n",
       " Row(MAL_ID='8', Name='Bouken Ou Beet', Score='6.98', Genders='Adventure, Fantasy, Shounen, Supernatural', English name='Beet the Vandel Buster', Japanese name='冒険王ビィト', Type='TV', Episodes='52', Aired='Sep 30, 2004 to Sep 29, 2005', Premiered='Fall 2004', Producers='TV Tokyo, Dentsu', Licensors='Unknown', Studios='Toei Animation', Source='Manga', Duration='23 min. per ep.', Rating='PG - Children', Ranked='3710.0', Popularity='4369', Members='13224', Favorites='18', Watching='642', Completed='7314', On-Hold='766', Dropped='1108', Plan to Watch='3394', Score-10='312.0', Score-9='529.0', Score-8='1242.0', Score-7='1713.0', Score-6='1068.0', Score-5='634.0', Score-4='265.0', Score-3='83.0', Score-2='50.0', Score-1='27.0')]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_titles = spark.read.csv('anime.csv', header=True)\n",
    "anime_titles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "nZGzLULlfzyf"
   },
   "outputs": [],
   "source": [
    "def name_retriever(anime_id, dataframe=anime_titles):\n",
    "    return anime_titles.where(anime_titles.MAL_ID == anime_id).take(1)[0]['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYYubwQOhoRT",
    "outputId": "24571bf9-a5cb-4cf3-e4e1-d5b65d9d9569"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name_retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1cf41d48bc5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Should show Cowboy Bepop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manime_titles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'name_retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# Should show Cowboy Bepop\n",
    "print(name_retriever(1, anime_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Recommendations\n",
    "\n",
    "We are going to go ahead and pull recommendations for users. Spark actually already has built in functions to get recommendations called recommendForUserSubset. There is also a more general function called recommendForAllUsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "S7_g3IsChqsv"
   },
   "outputs": [],
   "source": [
    "# Get a random user\n",
    "users = rec_data.select(als.getUserCol()).distinct().limit(1)\n",
    "# Get recommendations based on 10 nearest users\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "# Instantiate recommendations\n",
    "recs = userSubsetRecs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "K9t0mietidNG",
    "outputId": "cb17b1ec-cb11-4eac-e321-d26004a277da"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Kindaichi Shounen no Jikenbo Movie 1: Operazakan - Aratanaru Satsujin'"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use indexing to obtain the movie id of top predicted rated item\n",
    "first_recommendation = recs[0]['recommendations'][0][0]\n",
    "\n",
    "# use the name retriever function to get the values\n",
    "name_retriever(first_recommendation,anime_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0YNAeqzm2lf",
    "outputId": "47b18e3c-8b94-41a2-9024-5cc7a7ae6c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=3, recommendations=[Row(anime_id=28251, rating=21.12415885925293), Row(anime_id=40639, rating=18.66773223876953), Row(anime_id=3444, rating=18.416454315185547), Row(anime_id=928, rating=16.966115951538086), Row(anime_id=41260, rating=16.548372268676758)])]"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations = model.recommendForAllUsers(5)\n",
    "recommendations.where(recommendations.user_id == 3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set up a function for creating a new user getting and recommendations for them based on review scores we input for them. We can put the same scores into Anime Planet, and see how our recommendations compare to theirs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "wkb0iK3pnQ12"
   },
   "outputs": [],
   "source": [
    "def new_user_recs(user_id, new_ratings, rating_df, anime_title_df, num_recs):\n",
    "    # turn the new_recommendations list into a spark DataFrame\n",
    "    new_user_ratings = spark.createDataFrame(new_ratings, rating_df.columns)\n",
    "    \n",
    "    # combine the new ratings df with the rating_df\n",
    "    anime_ratings_combined = rating_df.union(new_user_ratings)\n",
    "    \n",
    "    # create an ALS model and fit it\n",
    "    als = ALS(maxIter=5, rank=10, regParam=0.01, userCol='user_id', itemCol='anime_id', ratingCol='rating',\n",
    "              coldStartStrategy='drop')\n",
    "    model = als.fit(anime_ratings_combined)\n",
    "    \n",
    "    # make recommendations for all users using the recommendForAllUsers method\n",
    "    recommendations = model.recommendForAllUsers(num_recs)\n",
    "    \n",
    "    # get recommendations specifically for the new user that has been added to the DataFrame\n",
    "    recs_for_user = recommendations.where(recommendations.user_id == user_id).take(1)\n",
    "\n",
    "    for ranking, (anime_id, rating) in enumerate(recs_for_user[0]['recommendations']):\n",
    "      anime_string = name_retriever(anime_id, anime_title_df)\n",
    "      print('Recommendation {}: {}  | predicted score: {}'.format(ranking+1, anime_string, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70kpfLJmnQ-p",
    "outputId": "cdda3258-f5b1-4cbb-8ad8-67e2e2266c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation 1: Kanojo ga Kanji wo Suki na Riyuu.  | predicted score: 16.453216552734375\n",
      "Recommendation 2: Universe  | predicted score: 15.597274780273438\n",
      "Recommendation 3: 1/100 Train Station  | predicted score: 14.501545906066895\n",
      "Recommendation 4: Attakai, Fuyu Canada  | predicted score: 13.245718002319336\n",
      "Recommendation 5: Code Geass: Hangyaku no Lelouch Picture Drama - Kiseki no Anniversary  | predicted score: 13.189346313476562\n",
      "Recommendation 6: 1/100 Shibuya Crossing  | predicted score: 12.995405197143555\n",
      "Recommendation 7: 1/100 Rice Planting  | predicted score: 12.851329803466797\n",
      "Recommendation 8: Kimagure Mercy  | predicted score: 12.703506469726562\n",
      "Recommendation 9: Re Boot  | predicted score: 12.488151550292969\n",
      "Recommendation 10: Tokimeki Runners  | predicted score: 12.397653579711914\n"
     ]
    }
   ],
   "source": [
    "user_id = 1000000\n",
    "user_ratings_1 = [(user_id,1,7),\n",
    "                  (user_id,2,7),\n",
    "                  (user_id,30,10),\n",
    "                  (user_id,32937,10),\n",
    "                  (user_id,8625,5),\n",
    "                  (user_id,203,10)]\n",
    "new_user_recs(user_id,\n",
    "             new_ratings=user_ratings_1,\n",
    "             rating_df=rec_data,\n",
    "             anime_title_df=anime_titles,\n",
    "             num_recs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hliC0lUdnRDd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOdrXvkDnRHC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
